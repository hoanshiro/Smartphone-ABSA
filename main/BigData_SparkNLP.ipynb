{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BigData_SparkNLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABSLcZmQdZXd",
        "outputId": "2cfda23b-5a89-4508-ceb4-d7c3084ebe20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.3 MB 31 kB/s \n",
            "\u001b[K     |████████████████████████████████| 142 kB 18.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 57.0 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install -q pyspark==3.2.0 spark-nlp==3.4.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "QKqAWRTCiXnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! apt-get update -qq\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz\n",
        "\n",
        "!tar xf spark-2.3.0-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.0-bin-hadoop2.7\"\n",
        "# ! java -version\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "! pip install --ignore-installed -q spark-nlp==2.7.5\n",
        "\n",
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start(spark23=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGPfDgfsdhRB",
        "outputId": "70493bf8-f054-4d68-cf18-932adf9c084f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |██▍                             | 10 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 20 kB 29.7 MB/s eta 0:00:01\r\u001b[K     |███████                         | 30 kB 34.0 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 40 kB 36.5 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 51 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 61 kB 28.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 71 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 81 kB 29.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 92 kB 31.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 102 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 112 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 122 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 133 kB 32.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 139 kB 32.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.session import SparkSession\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import functions as f\n",
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.sql.functions import col, when\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml.feature import StopWordsRemover, RegexTokenizer\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer, Word2Vec\n",
        "from pyspark.sql import Row\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier"
      ],
      "metadata": {
        "id": "t-Qjj_8PiV1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start(gpu = True, spark23=True) # for GPU training >> sparknlp.start(gpu = True) # for Spark 2.3 =>> sparknlp.start(spark23 = True)\n",
        "\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline\n",
        "import pandas as pd\n",
        "import os\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "Mj8aaWLUd_5r",
        "outputId": "4cb99bbf-03dc-4e8a-f9d2-03a4c154daf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fb42b55b890>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://673ce0b17858:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.3.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hoanshiro/Smartphone-ABSA"
      ],
      "metadata": {
        "id": "1LceNhdKd1cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Smartphone-ABSA"
      ],
      "metadata": {
        "id": "pNa6kiTyeY6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('data/processed_train.csv')\n",
        "test_data =  pd.read_csv('data/processed_test.csv')\n",
        "valid_data =  pd.read_csv('data/processed_valid.csv')"
      ],
      "metadata": {
        "id": "pcLVDP8webxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " train_data = pd.concat([train_data, valid_data], axis=0)"
      ],
      "metadata": {
        "id": "hD40oJlIeghe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop N/A\n",
        "train_sentiment_data = train_data.dropna()\n",
        "test_sentiment_data = test_data.dropna()"
      ],
      "metadata": {
        "id": "nYHucdqxeisy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentiment_data"
      ],
      "metadata": {
        "id": "H6IF0TclepCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#[toxic, obscene]\n",
        "labels = list(train_data.columns[2:])\n",
        "def convert2label(row):\n",
        "  concat_label = ''\n",
        "  for i in range(len(row)):\n",
        "    if str(row[i]) in ['1', '1.0']:\n",
        "      concat_label = concat_label + labels[i] + ', '\n",
        "  concat_label = concat_label[:-2] + ''\n",
        "  return concat_label"
      ],
      "metadata": {
        "id": "QKcFCyelekoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentiment_data['labels'] = train_sentiment_data[labels].apply(lambda row: convert2label(row), axis = 1)\n",
        "test_sentiment_data['labels'] = test_sentiment_data[labels].apply(lambda row: convert2label(row), axis = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6wAjJ-4hl2l",
        "outputId": "daee2fdb-7885-4f5b-d9b3-7646f52b0e0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for df in [train_sentiment_data, test_sentiment_data]:\n",
        "  df['labels'] = df[labels].apply(lambda row: convert2label(row), axis = 1)\n",
        "\n",
        "df_train_final = train_sentiment_data[['text', 'labels']]\n",
        "df_test_final = test_sentiment_data[['text', 'labels']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvYJKOOye0AN",
        "outputId": "32a5ad96-7afe-4ecf-8bfe-a5ce289c5e84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_final"
      ],
      "metadata": {
        "id": "p3B2ijWrkCWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Pandas to PySpark DataFrame, train_sentiment_data\n",
        "Train = spark.createDataFrame(df_train_final)\n",
        "# Convert Pandas to PySpark DataFrame, test_sentiment_data\n",
        "Test = spark.createDataFrame(df_test_final) "
      ],
      "metadata": {
        "id": "zzw9GMyVe2qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split, col\n",
        "Train = Train.withColumn('labels', split(col(\"labels\"),\",\"))\n",
        "#\n",
        "from pyspark.sql.functions import split, col\n",
        "Test = Test.withColumn('labels', split(col(\"labels\"),\",\"))"
      ],
      "metadata": {
        "id": "5hyRq_Sle4ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document = DocumentAssembler()\\\n",
        "      .setInputCol(\"text\")\\\n",
        "      .setOutputCol(\"document\")\\\n",
        "      .setCleanupMode(\"shrink\")\n",
        "\n",
        "bert_sent = BertSentenceEmbeddings.pretrained(\"bert_embeddings_bert_base_vi_cased\",\"vi\") \\\n",
        "      .setInputCols([\"document\"])\\\n",
        "      .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "multiClassifier = MultiClassifierDLApproach()\\\n",
        "      .setInputCols(\"sentence_embeddings\")\\\n",
        "      .setOutputCol(\"category\")\\\n",
        "      .setLabelColumn(\"labels\")\\\n",
        "      .setBatchSize(64)\\\n",
        "      .setMaxEpochs(100)\\\n",
        "      .setLr(1e-5)\\\n",
        "      .setThreshold(0.5)\\\n",
        "      .setShufflePerEpoch(True)\\\n",
        "      .setEnableOutputLogs(True)\\\n",
        "      .setValidationSplit(0.1)\n",
        "\n",
        "pipeline = Pipeline(\n",
        "    stages = [\n",
        "        document,\n",
        "        bert_sent,\n",
        "        multiClassifier\n",
        "    ])"
      ],
      "metadata": {
        "id": "rBRV19z2e6vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -r /root/annotator_logs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWepb66Pe8Tu",
        "outputId": "7a3eac97-ede3-4b4a-fd76-1989232f0baf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/root/annotator_logs': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "pipelineModel = pipeline.fit(Train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvtpJArYjXZT",
        "outputId": "595fa33d-6021-4cdf-cc1e-6ea5661cb7e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.83 s, sys: 311 ms, total: 3.14 s\n",
            "Wall time: 9min 31s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = pipelineModel.transform(Test)"
      ],
      "metadata": {
        "id": "qryoNleHjY4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We are going to use sklearn to evalute the results on test dataset\n",
        "test_preds = preds.select('labels','text',\"category.result\").toPandas()"
      ],
      "metadata": {
        "id": "vfj5o--_kq3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Prediction = spark.createDataFrame(test_preds)\n",
        "Prediction.show()"
      ],
      "metadata": {
        "id": "kzaNvIB8ktNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#[toxic, obscene]\n",
        "labels = list(train_data.columns[2:])\n",
        "def labels2index(row):\n",
        "  results = row['result']\n",
        "  # print(str(results))\n",
        "  results = [result.replace(' ', '') for result in results]\n",
        "  for label in results:\n",
        "    if label != '':\n",
        "      row[label] = 1\n",
        "  return row[:-1]"
      ],
      "metadata": {
        "id": "9o8ZfjQLkweh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = test_preds.apply(lambda row: labels2index(row), axis=1)\n",
        "test"
      ],
      "metadata": {
        "id": "N8GXOL31kyRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_data.iloc[:, 2:], test.iloc[:, 2:], target_names=labels))"
      ],
      "metadata": {
        "id": "a0vRVAx4kzUX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}